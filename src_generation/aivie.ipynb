{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Cycle-GAN in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this notebook on Google COLAB: <br>\n",
    "1. Change runtime type to GPU<br>\n",
    "2. Execute all cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A pytorch implementation of the Cycle-GAN architecture used for generating art in aivie.\n",
    "The original paper by Zhu et al. can be found at: https://arxiv.org/pdf/1703.10593.pdf\n",
    "This implementation is based on their more complete and performant official pytorch implementation, which can be found at: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\n",
    "This implementation was inspired by https://github.com/aitorzip/PyTorch-CycleGAN. However, this implementation is not a verbatim copy of the aforementioned implementation, as it was heavily modified and completely rewritten for practical and educational purposes. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip /content/data_img_A.zip -d /content/data_img_A\n",
    "!unzip /content/data_img_B.zip -d /content/data_img_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torch.utils.data as data\n",
    "import torch.autograd as autograd\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual block used in the generator of this Cycle-GAN.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, r_padding: int = 1, kernel_size : int = 3):\n",
    "        \"\"\"\n",
    "        Creates a ResNetBlock instance\n",
    "        \n",
    "        Args:\n",
    "            in_channels (int) : number of channels in input image\n",
    "            r_padding (int) : size of padding for left, right, top and bottom for torch.nn.ReflectionPad2d. Default is 1.\n",
    "            kernel_size (int) : height and width for the 2D convolutional window in torch.nn.Conv2d layer. Default is 3.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        #set in_channels and out_channels to be the same for convolutional layers\n",
    "        self._out_channels = in_channels \n",
    "        \n",
    "        #build model\n",
    "        self.model = nn.Sequential(\n",
    "            nn.ReflectionPad2d(padding = r_padding),\n",
    "            nn.Conv2d(in_channels = in_channels, out_channels = self._out_channels, kernel_size=kernel_size),\n",
    "            nn.InstanceNorm2d(num_features=in_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(padding = r_padding),\n",
    "            nn.Conv2d(in_channels = in_channels, out_channels = self._out_channels, kernel_size=kernel_size),\n",
    "            nn.InstanceNorm2d(num_features=in_channels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Concatenates tensors in forward\"\"\"\n",
    "        return x + self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"Generator for the Cycle-GAN\"\"\"\n",
    "\n",
    "    def __init__(self, start_in: int = 3, start_out: int = 64, end_out: int = 3, ends_kernel_size: int = 7, mid_kernel_size: int = 3, r_padding: int = 3, padding: int = 1, stride: int = 2, n_resnet = 9) -> None:\n",
    "        \"\"\"\n",
    "        Creates a Generator instance. Will have c7s1-64, d128, d256, R256, R256, R256, R256, R256,R256, R256, R256, R256, u256, u128, c7s1-3 architecture. \n",
    "\n",
    "        Args:\n",
    "            start_in (int): number of channels in input tensor. Default is 3.\n",
    "            start_out (int): number of channels produced by first torch.nn.Conv2d layer. Default is 64.\n",
    "            end_out (int): number of channels in final tensor. Default is 3.\n",
    "            ends_kernel_size (int): height and width for the 2D convolutional window in first and last torch.nn.Conv2d layer. Default is 7.\n",
    "            mid_kernel_size (int): height and width for the 2D convolutional window in middle torch.nn.Conv2d layers. Default is 3.\n",
    "            r_padding (int) : size of padding for left, right, top and bottom for torch.nn.ReflectionPad2d. Default is 3.\n",
    "            padding (int): size of zero-padding in torch.nn.Conv2d layer. Default is 1.\n",
    "            stride (int) : stride argument for filter in torch.nn.Conv2d layer. Default is 2.\n",
    "            n_resnet (int) : determines the number of resnet blocks in model. Default is 9.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # define constants\n",
    "        self.NUM_DOWNSAMPLE = 2\n",
    "        self.NUM_UPSAMPLE = 2\n",
    "\n",
    "        # c7s1-64 block\n",
    "        self._arg_model = [\n",
    "            nn.ReflectionPad2d(padding=r_padding),\n",
    "            nn.Conv2d(in_channels = start_in, out_channels = start_out, kernel_size = ends_kernel_size),\n",
    "            nn.InstanceNorm2d(num_features=start_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "\n",
    "        self._in_channels = start_in\n",
    "        self._out_channels = start_out\n",
    "\n",
    "        #d128 & d256 block\n",
    "        for _ in range(self.NUM_DOWNSAMPLE):\n",
    "            self._in_channels = self._out_channels\n",
    "            self._out_channels *= 2\n",
    "            self._arg_model += self._downsample(in_channels=self._in_channels, out_channels=self._out_channels, padding = padding, kernel_size=mid_kernel_size, stride=stride)\n",
    "\n",
    "        #R256 blocks\n",
    "        for _ in range(n_resnet):\n",
    "            self._arg_model.append(ResNetBlock(in_channels=self._out_channels))\n",
    "        \n",
    "        \n",
    "        #u128 & u64 blocks\n",
    "        for _ in range(self.NUM_UPSAMPLE):\n",
    "            self._in_channels = self._out_channels\n",
    "            self._out_channels = self._in_channels // 2\n",
    "            self._arg_model += self._upsample(in_channels=self._in_channels, out_channels=self._out_channels, kernel_size=mid_kernel_size, padding=padding, output_padding=padding, stride = stride)\n",
    "\n",
    "        #output layer\n",
    "        self._arg_model += [\n",
    "            nn.ReflectionPad2d(padding=r_padding),\n",
    "            nn.Conv2d(in_channels = self._out_channels, out_channels=end_out, kernel_size=ends_kernel_size),\n",
    "            nn.Tanh()\n",
    "        ]\n",
    "\n",
    "        #build model\n",
    "        self.model = nn.Sequential(*self._arg_model)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Standard forward\"\"\"\n",
    "        return self.model(x)\n",
    "\n",
    "    \n",
    "    def _downsample(self, in_channels: int, out_channels: int, padding: int, kernel_size: int, stride: int) -> list:\n",
    "        \"\"\"\n",
    "        Creates downsampling block for generator\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): number of channels in input tensor\n",
    "            out_channels (int): number of channels produced by torch.nn.Conv2d layer\n",
    "            padding (int): size of zero-padding in torch.nn.Conv2d layer.\n",
    "            kernel_size (int): height and width for the 2D convolutional window in torch.nn.Conv2d layer.\n",
    "            stride (int) : stride argument for filter in torch.nn.Conv2d layer.\n",
    "        Returns:\n",
    "            list: list with torch.nn.Conv2d, torch.nn.InstanceNorm2d, nn.ReLU\n",
    "        \"\"\"\n",
    "        cur = [\n",
    "            nn.Conv2d(in_channels= in_channels, out_channels=out_channels, kernel_size=kernel_size, padding = padding, stride=stride),\n",
    "            nn.InstanceNorm2d(num_features=out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "        return cur\n",
    "\n",
    "\n",
    "    def _upsample(self, in_channels: int, out_channels: int, kernel_size: int, padding: int, output_padding: int, stride: int) -> list:\n",
    "        \"\"\"\n",
    "        Creates upsampling block for generator\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): number of channels in input tensor\n",
    "            out_channels (int): number of channels produced by torch.nn.Conv2d layer\n",
    "            padding (int): size of zero-padding in torch.nn.Conv2d layer.\n",
    "            output_padding (int) : controls additional size added to output of nn.ConvTranspose2d\n",
    "            stride (int) : stride argument for filter in torch.nn.Conv2d layer.\n",
    "            kernel_size (int): height and width for the 2D convolutional window in torch.nn.ConvTranspose2d layer.\n",
    "        Returns:\n",
    "            list: list with torch.nn.ConvTranspose2d, torch.nn.InstanceNorm2d, nn.ReLU\n",
    "        \"\"\"\n",
    "        cur = [\n",
    "            nn.ConvTranspose2d(in_channels= in_channels, out_channels=out_channels, kernel_size=kernel_size, padding = padding, output_padding=output_padding, stride=stride),\n",
    "            nn.InstanceNorm2d(num_features=out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "        return cur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Discriminator for the Cycle-GAN\"\"\"\n",
    "\n",
    "    def __init__(self, start_in: int = 3, start_out: int = 64, kernel_size: int = 4, padding: int = 1, stride: int = 2, negative_slope: int = 0.2, num_groups: int = 4) -> None:\n",
    "        \"\"\"\n",
    "        Creates Discriminator instance\n",
    "\n",
    "        Args:\n",
    "            start_in (int): number of channels in input tensor. Default is 3.\n",
    "            start_out (int): number of channels produced by first torch.nn.Conv2d layer. Default is 64.\n",
    "            kernel_size (int): height and width for the 2D convolutional window in torch.nn.Conv2d layer. Default is 4.\n",
    "            padding (int): size of zero-padding in torch.nn.Conv2d layer. Default is 1.\n",
    "            stride (int): stride argument for filter in torch.nn.Conv2d layer. Default is 2.\n",
    "            negative_slope (int): determines the negative slope of the torch.nn.LeakyReLU layer. Default is 0.2\n",
    "            num_groups (int): number of convolutional groups in model. Default is 4.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        #creates a list to be passed into torch.nn.Sequential\n",
    "        self._arg_model = self._build_conv_groups(in_channels=start_in, out_channels=start_out, kernel_size = kernel_size, padding = padding, stride = stride, negative_slope = negative_slope, normalization=False)\n",
    "        \n",
    "        self._in_channels = start_in\n",
    "        self._out_channels = start_out\n",
    "\n",
    "        #add groups with normalization to model\n",
    "        for _ in range(num_groups-2):\n",
    "            self._in_channels = self._out_channels\n",
    "            self._out_channels *= 2\n",
    "            self._arg_model += self._build_conv_groups(in_channels=self._in_channels, out_channels=self._out_channels, kernel_size = kernel_size, padding = padding, stride = stride, negative_slope = negative_slope)\n",
    "        \n",
    "        #add final conv group to model\n",
    "        self._in_channels = self._out_channels\n",
    "        self._out_channels *= 2\n",
    "        self._arg_model += self._build_conv_groups(in_channels=self._in_channels, out_channels=self._out_channels, kernel_size = kernel_size, padding = padding, stride = stride, negative_slope = negative_slope, has_stride=False)\n",
    "\n",
    "        #add dense classification layer\n",
    "        self._arg_model.append(nn.Conv2d(in_channels = self._out_channels, out_channels = 1, kernel_size = kernel_size, padding = padding))\n",
    "\n",
    "        self.model = nn.Sequential(*self._arg_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward and flatten\"\"\"\n",
    "        x = self.model(x)\n",
    "        return F.avg_pool2d(x, x.size()[2:]).view(x.size()[0], -1)\n",
    "\n",
    "    def _build_conv_groups(self, in_channels: int, out_channels: int, kernel_size: int, padding: int, stride: int, negative_slope: int, normalization: bool  = True, has_stride: bool = True) -> list:\n",
    "        \"\"\"\n",
    "        Builds convolutional 'group' consisting of torch.nn.Conv2d, (torch.nn.InstanceNorm2d), and torch.nn.LeakyReLU\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): number of channels in input tensor\n",
    "            out_channels (int): number of channels produced by torch.nn.Conv2d layer.\n",
    "            kernel_size (int): height and width for the 2D convolutional window in torch.nn.Conv2d layer. \n",
    "            padding (int): size of zero-padding in torch.nn.Conv2d layer. \n",
    "            stride (int): stride argument for filter in torch.nn.Conv2d layer. \n",
    "            normalization (bool): determines whether or not the group will have normalization. Normalization if True, else no normalization.\n",
    "            negative_slope (int): determines the negative slope of the torch.nn.LeakyReLU layer.\n",
    "            has_stride (bool): determines whether the torch.nn.Conv2d layers has a stride. Default is True.\n",
    "\n",
    "        Returns:\n",
    "            list : list with a torch.nn.Conv2d, (torch.nn.InstanceNorm2d), and torch.nn.LeakyReLU to be a component of full Discriminator\n",
    "        \"\"\"\n",
    "        cur = []\n",
    "\n",
    "        #appends convolutional layer\n",
    "        if has_stride:\n",
    "            cur.append(nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride = stride, padding = padding))\n",
    "        else:\n",
    "            cur.append(nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, padding = padding))\n",
    "\n",
    "        #appends normalization layer\n",
    "        if normalization:\n",
    "            cur.append(nn.InstanceNorm2d(num_features = out_channels))\n",
    "\n",
    "        #appends LeakyReLU activation layer\n",
    "        cur.append(nn.LeakyReLU(negative_slope = negative_slope, inplace = True))\n",
    "\n",
    "        return cur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetAB(data.Dataset):\n",
    "    \"\"\"Dataset used for images in class A and class B\"\"\"\n",
    "    \n",
    "    def __init__(self, *datasets) -> None:\n",
    "        \"\"\"Creates DatasetAB instance\"\"\"\n",
    "        self.datasets = datasets\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns length of max dataset\"\"\"\n",
    "        length = [len(d) for d in self.datasets]\n",
    "\n",
    "        return max(length)\n",
    "\n",
    "    def __getitem__(self, index:int) -> tuple:\n",
    "        \"\"\"Returns an element from dataset A and B\"\"\"\n",
    "        d = [dataset for dataset in self.datasets]\n",
    "        assert len(d) == 2\n",
    "\n",
    "        a = d[0][index%len(d[0])]\n",
    "        b = d[1][index%len(d[1])]\n",
    "        \n",
    "        return a[0], b[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CycleGAN:\n",
    "    \"\"\"complete Cycle-GAN class with two generators and two discriminators\"\"\"\n",
    "\n",
    "    def __init__(self, lr: int = 0.0002, trainable: bool = False, lambda_a: float = 10.0, lambda_b: float = 10.0, lambda_identity: float = 0.5) -> None:\n",
    "        \"\"\"\n",
    "        Creates Cycle-GAN instance\n",
    "        \n",
    "        Args:\n",
    "            lr (int): learning rate for torch.optim.Adam in CycleGAN. Default is 0.0002.\n",
    "            trainable (bool): determines whether or not the model can be trained. True for training, False for generating. Default is False\n",
    "            lambda_a (float):\n",
    "            lambda_b (float):\n",
    "            lambda_identity (float): \n",
    "        \"\"\"\n",
    "\n",
    "        self.lr = lr\n",
    "        self.trainable = trainable\n",
    "        self.lambda_a = lambda_a\n",
    "        self.lambda_b = lambda_b\n",
    "        self.lambda_identity = lambda_identity\n",
    "\n",
    "        # set up torch device if GPU is available\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # define generator and discriminator\n",
    "        # generator and discriminator can be adjusted by overriding default params.\n",
    "        self.generator_A2B = Generator().to(self.device)\n",
    "        self.discriminator_A = Discriminator().to(self.device)\n",
    "        self.generator_B2A = Generator().to(self.device)\n",
    "        self.discriminator_B = Discriminator().to(self.device)\n",
    "\n",
    "        #define loss functions for generators and discriminators\n",
    "        self.criterion_idt = nn.L1Loss()\n",
    "        self.criterion_cycle = nn.L1Loss()\n",
    "        self.criterion_gan = nn.MSELoss()\n",
    "\n",
    "        #optimizers for discriminators and generators\n",
    "        self.optim_discriminator_A = optim.Adam(self.discriminator_A.parameters(), lr = self.lr, betas=(0.5, 0.999))\n",
    "        self.optim_discriminator_B = optim.Adam(self.discriminator_B.parameters(), lr = self.lr, betas=(0.5, 0.999))\n",
    "        self.optim_generator = optim.Adam(itertools.chain(self.generator_A2B.parameters(), self.generator_B2A.parameters()), lr = self.lr, betas=(0.5, 0.999))\n",
    "\n",
    "        #data handling\n",
    "        self.data_loader = None\n",
    "    \n",
    "    def load_data(self, path_A: str, path_B: str, batch_size: int = 32) -> None:\n",
    "        \"\"\"\n",
    "        Loads data into the model\n",
    "\n",
    "        Args:\n",
    "            path_A (str): path to the directory containing the data for A\n",
    "            path_B (str): path to the directory containing the data for B\n",
    "            batch_size (int): batch size for the data. Default is 32.\n",
    "        \"\"\"\n",
    "\n",
    "        #transformations to be applied to data. Modify as needed\n",
    "        transformations = transforms.Compose(transforms=[\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean = (0.5,0.5,0.5), std=(0.5,0.5,0.5))\n",
    "        ])\n",
    "\n",
    "        #apply transformations to image in given directory.\n",
    "        images = DatasetAB(\n",
    "            datasets.ImageFolder(path_A, transform=transformations),\n",
    "            datasets.ImageFolder(path_B, transform=transformations)       \n",
    "        )\n",
    "        #load dataset into dataloader for use in training\n",
    "        self.data_loader = data.DataLoader(\n",
    "            dataset=images,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        #ground truth for fake and real images\n",
    "        self.target_reals = torch.Tensor(np.ones(batch_size)).to(self.device)\n",
    "        self.target_fakes = torch.Tensor(np.zeros(batch_size)).to(self.device)\n",
    "\n",
    "    def train(self, epochs: int = 50) -> None:\n",
    "        \"\"\"\n",
    "        Trains Cycle-GAN model\n",
    "\n",
    "        Args:\n",
    "            epochs (int): number of epochs to train Cycle-GAN model for. Default is 50.\n",
    "        \"\"\"\n",
    "        #raise necessary errors\n",
    "        if not self.trainable:\n",
    "            raise RuntimeError('Cannot train model when trainable is set to False')\n",
    "        if self.data_loader is None:\n",
    "            raise RuntimeError('No data loaded into the model for training')\n",
    "        \n",
    "        # iterate through epochs\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            t = tqdm(iter(self.data_loader), leave = False, total=len(self.data_loader))\n",
    "            #iterate through batches\n",
    "            for _, batch in enumerate(t):\n",
    "\n",
    "                #current batch             \n",
    "                batch_real_a = autograd.Variable(batch[0]).to(self.device)\n",
    "                batch_real_b = autograd.Variable(batch[1]).to(self.device)\n",
    "                \n",
    "                #generator training\n",
    "                self.optim_generator.zero_grad()\n",
    "\n",
    "                #GAN losses\n",
    "                fake_b = self.generator_A2B(batch_real_a)\n",
    "                pred_fake = self.discriminator_B(fake_b)\n",
    "                generator_A2B_loss = self.criterion_gan(pred_fake, self.target_reals)\n",
    "\n",
    "                fake_a = self.generator_B2A(batch_real_b)\n",
    "                pred_fake = self.discriminator_A(fake_a)\n",
    "                generator_B2A_loss = self.criterion_gan(pred_fake, self.target_reals)\n",
    "\n",
    "                #forward and backward cycle losses\n",
    "                reconstructed_a = self.generator_B2A(fake_b)\n",
    "                cycle_ABA_loss = self.criterion_cycle(reconstructed_a, batch_real_a) * self.lambda_a\n",
    "\n",
    "                reconstructed_b = self.generator_A2B(fake_a)\n",
    "                cycle_BAB_loss = self.criterion_cycle(reconstructed_b, batch_real_b) * self.lambda_b\n",
    "\n",
    "                #identity losses\n",
    "                identity_a = self.generator_B2A(batch_real_a)\n",
    "                identity_a_loss = self.criterion_idt(identity_a, batch_real_a) * self.lambda_a * self.lambda_identity\n",
    "\n",
    "                identity_b = self.generator_A2B(batch_real_b)\n",
    "                identity_b_loss = self.criterion_idt(identity_b, batch_real_b) * self.lambda_b * self.lambda_identity\n",
    "\n",
    "                generator_losses = generator_A2B_loss + generator_B2A_loss + cycle_ABA_loss + cycle_BAB_loss + identity_a_loss + identity_b_loss\n",
    "                generator_losses.backward()\n",
    "\n",
    "                self.optim_generator.step()\n",
    "\n",
    "                #discriminator training\n",
    "                self.optim_discriminator_A.zero_grad()\n",
    "\n",
    "                pred_real = self.discriminator_A(batch_real_a)\n",
    "                loss_d_real = self.criterion_gan(pred_real, self.target_reals)\n",
    "\n",
    "                pred_fake = self.discriminator_A(fake_a.detach())\n",
    "                loss_d_fake = self.criterion_gan(pred_fake, self.target_fakes)\n",
    "\n",
    "                loss_discriminator_a = 0.5 * (loss_d_real + loss_d_fake)\n",
    "                loss_discriminator_a.backward()\n",
    "\n",
    "                self.optim_discriminator_A.step()\n",
    "\n",
    "                self.optim_discriminator_B.zero_grad()\n",
    "\n",
    "                pred_real = self.discriminator_A(batch_real_a)\n",
    "                loss_d_real = self.criterion_gan(pred_real, self.target_reals)\n",
    "\n",
    "                pred_fake = self.discriminator_B(fake_b.detach())\n",
    "                loss_d_fake = self.criterion_gan(pred_fake, self.target_fakes)\n",
    "\n",
    "                loss_discriminator_b = 0.5 * (loss_d_real + loss_d_fake)\n",
    "                loss_discriminator_b.backward()\n",
    "\n",
    "                self.optim_discriminator_B.step()\n",
    "                \n",
    "            torch.save(self.generator_A2B.state_dict(), '/content/generator_A2B.pth')\n",
    "            torch.save(self.generator_B2A.state_dict(), '/content/generator_B2A.pth')\n",
    "            torch.save(self.discriminator_A.state_dict(), '/content/discriminator_A.pth')\n",
    "            torch.save(self.discriminator_B.state_dict(), '/content/discriminator_B.pth')\n",
    "\n",
    "    def generate(self, img_dir: str, out_dir: str, direction: str) -> None:\n",
    "        \"\"\"\n",
    "        Generates images using trained Cycle-GAN model.\n",
    "        img_dir (str): directory from which images will be generated\n",
    "        out_dir (str): directory where generated images will be outputted. \n",
    "        direction (str): direction of image generation. Valid arguments are 'A2B' and 'B2A'\n",
    "        \"\"\"\n",
    "        #A2B\n",
    "        if direction.lower() == 'a2b':\n",
    "            temp_generator = Generator()\n",
    "\n",
    "            #load in model from same .pth file path as in train() method\n",
    "            temp_generator.load_state_dict(torch.load('/content/generator_A2B.pth'))\n",
    "\n",
    "            #load dataset into dataloader for use in generating\n",
    "            temp_data_loader = data.DataLoader(\n",
    "                dataset=datasets.ImageFolder(img_dir, transform=self.transformations),\n",
    "                batch_size=1,\n",
    "                shuffle=True,\n",
    "                pin_memory=True\n",
    "            )\n",
    "            with torch.no_grad():\n",
    "                for index, batch in enumerate(temp_data_loader):\n",
    "                    #generate image.\n",
    "                    img = (temp_generator(batch[0]).data + 1)/2.0\n",
    "\n",
    "                    #save image in directory\n",
    "                    utils.save_image(img, os.path.join(out_dir, '{}.png'.format(index)))\n",
    "\n",
    "        #B2A\n",
    "        elif direction.lower() == 'b2a':\n",
    "            temp_generator = Generator()\n",
    "\n",
    "            #load in model from same .pth file path as in train() method\n",
    "            temp_generator.load_state_dict(torch.load('/content/generator_B2A.pth'))\n",
    "            \n",
    "            #load dataset into dataloader for use in generating\n",
    "            temp_data_loader = data.DataLoader(\n",
    "                dataset=datasets.ImageFolder(img_dir, transform=self.transformations),\n",
    "                batch_size=1,\n",
    "                shuffle=True,\n",
    "                pin_memory=True\n",
    "            )\n",
    "            with torch.no_grad():\n",
    "                for index, batch in enumerate(temp_data_loader):\n",
    "                    #generate image.\n",
    "                    img = (temp_generator(batch[0]).data + 1)/2.0\n",
    "\n",
    "                    #save image in directory\n",
    "                    utils.save_image(img, os.path.join(out_dir, '{}.png'.format(index)))\n",
    "\n",
    "        #raise error if invalid direction is passed\n",
    "        else:\n",
    "            raise ValueError('{} is not a valid direction'.format(direction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = CycleGAN()\n",
    "c.trainable = True\n",
    "c.load_data(path_A='/content/data_img_A', path_B='/content/data_img_B', batch_size=8)\n",
    "c.train(epochs=75)\n",
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}